{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661020893946,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "ttSG4wFVEata"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dleir\\miniconda3\\envs\\tfm\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1661020820703,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "WNFp1G8sW_jv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('poems_cleaned.csv')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "#test_set = df.sample(n = 200)\n",
    "#df = df.loc[~df.index.isin(test_set.index)]\n",
    "test_set = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661020821016,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "AbGe08klgmsI"
   },
   "outputs": [],
   "source": [
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1661020821352,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "4Gjs8Mk7gqaV"
   },
   "outputs": [],
   "source": [
    "#test_set['True_end_poems'] = test_set['poem'].str.split().str[-20:].apply(' '.join)\n",
    "#test_set['poem'] = test_set['poem'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67853,
     "status": "ok",
     "timestamp": 1661020890183,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "XHdeV_N5hO5Q",
    "outputId": "6a7c7b5e-522a-44d2-bf0b-d16c29cfca2e"
   },
   "outputs": [],
   "source": [
    "class SpanishPoems(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"DeepESP/gpt2-spanish\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.poems = []\n",
    "\n",
    "        for row in df['poem']:\n",
    "          self.poems.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))               \n",
    "        if truncate:\n",
    "            self.poems = self.poems[:20000]\n",
    "        self.poems_count = len(self.poems)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.poems_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.poems[item]\n",
    "    \n",
    "dataset = SpanishPoems(df['poem'], truncate=True, gpt2_type=\"DeepESP/gpt2-spanish\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2950,
     "status": "ok",
     "timestamp": 1661020893094,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "dIbrayvfi77s",
    "outputId": "b89d8af3-9982-4943-82a4-c26bd45804a9"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('DeepESP/gpt2-spanish')\n",
    "model = GPT2LMHeadModel.from_pretrained('DeepESP/gpt2-spanish')\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1661020903194,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "ZimJ0dmcjQ5Y"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=5, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\"./spanish_poems_model\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6903807,
     "status": "ok",
     "timestamp": 1661027825548,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "vnrP1RdWj5T4",
    "outputId": "5805598d-4c86-4ca5-fa1a-346fc5ff11ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dleir\\miniconda3\\envs\\tfm\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [15:34, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(0.8953, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [15:37, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(1.1394, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [15:38, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(1.1098, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [15:47, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(1.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [16:25, 20.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2352,
     "status": "ok",
     "timestamp": 1661028019723,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "rzZ0qoT8j944"
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"spanish_poems_model_v1.0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1661029762880,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "OiXaY4If_CON"
   },
   "outputs": [],
   "source": [
    "prueba = \"\"\"\n",
    "No sé, dicen que es invierno\n",
    "y que fuera está nevando.\n",
    "Pero aquí sigue siendo verano.\n",
    "\n",
    "Tú que te coronaste en Cotos sin necesidad de vestido,\n",
    "o de trampas en papel de efecto retardado,\n",
    "querías saber lo que me hizo el sol en el Atlántico\n",
    "pero tú te quemaste la espalda\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1661029030334,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "OnO6hMoW_cID"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1661031209641,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "-Q3YzHwpC5dl"
   },
   "outputs": [],
   "source": [
    "def text_generation(text, temperature=1., top_p=0.8, entry_length=30):\n",
    "  x = generate(model.to('cpu'), tokenizer, text, entry_count=1, temperature=temperature, top_p=top_p, entry_length=entry_length)\n",
    "  print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32077,
     "status": "ok",
     "timestamp": 1661031985398,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "6HK_1vutDI7b",
    "outputId": "f487c343-7832-404d-cd2a-4c9ce4d29ff6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [02:49<00:00, 169.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No sé, dicen que es invierno\n",
      "y que fuera está nevando.\n",
      "Pero aquí sigue siendo verano.\n",
      "\n",
      "Tú que te coronaste en Cotos sin necesidad de vestido,\n",
      "o de trampas en papel de efecto retardado,\n",
      "querías saber lo que me hizo el sol en el Atlántico\n",
      "pero tú te quemaste la espalda\n",
      "ya, tú, que en la tierra tienes manos de pintor,\n",
      "\n",
      "ya, claro, y que con la memoria has de abrir la cabeza. \n",
      "\n",
      "Así que te sacudiste en la bañera,\n",
      "yo estoy con mi espejo de agua,\n",
      "\n",
      "¿de qué manera? ¿Tú que me dijiste que me preguntaste? \n",
      "\n",
      "Tú no me lo dijiste, ¿eh? \n",
      "\n",
      "Yo te dije que me disculpaste\n",
      "¿para qué? Ya lo sabes. \n",
      "\n",
      "Ya, claro, ya. ¡No! \n",
      "\n",
      "No, no, no me lo dijiste. No me lo dijiste. \n",
      "\n",
      "Ni siquiera me dijiste una palabra,\n",
      "ni me respondiste una sola vez,\n",
      "\n",
      "ni me has contestado nada. \n",
      "<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1661032293187,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "JoTFD2i-M1KV"
   },
   "outputs": [],
   "source": [
    "prueba = \"\"\"\n",
    "Es porque aún somos pequeños.\n",
    "\n",
    "Tú no dejabas de mirar por la ventana,\n",
    "como si las golondrinas fueran a bailar por ti.\n",
    "Irradiabas el aura y la luz de la vida\n",
    "la mirada de una niña que ha visto más de lo que quería\n",
    "por esa ventana que mostraba en el atardecer un nuevo alba.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32000,
     "status": "ok",
     "timestamp": 1661032328116,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "sXIpQ1nbO9iv",
    "outputId": "56a658c1-f0d8-49fa-c154-4f166f75a340"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [02:09<00:00, 129.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Es porque aún somos pequeños.\n",
      "\n",
      "Tú no dejabas de mirar por la ventana,\n",
      "como si las golondrinas fueran a bailar por ti.\n",
      "Irradiabas el aura y la luz de la vida\n",
      "la mirada de una niña que ha visto más de lo que quería\n",
      "por esa ventana que mostraba en el atardecer un nuevo alba.\n",
      "¿Entiendes ahora por qué has venido a verte? \n",
      "\n",
      "Porque yo soy la mirada de la que ha visto lo que ha visto. \n",
      "\n",
      "¿Qué te ha ocurrido? \n",
      "\n",
      "Que ya es tarde. \n",
      "\n",
      "Pero tú has venido a verte. \n",
      "\n",
      "Tú no has venido a verme. \n",
      "\n",
      "¿Qué quieres decir? \n",
      "\n",
      "No me has venido a mí. \n",
      "\n",
      "No me has venido a mí. \n",
      "\n",
      "No me has venido a mí. \n",
      "\n",
      "Es que no me has venido a mí. \n",
      "\n",
      "Pero sí te has venido a mí. \n",
      "\n",
      "No me<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=0.9, top_p=0.8, entry_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OKduKsCHO_Xu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:03<00:00, 63.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "y cómo navegaban,\n",
      "\n",
      "sus largos dedos hacían el camino,\n",
      "delante de los trajes de vuelo\n",
      "hacia la ventana\n",
      "de una foto sin fecha, en un ambiente\n",
      "con calma\n",
      "y con la verdad se veía cómo llegaba\n",
      "el tren, nos ponía a la carretera,\n",
      "los ferrocarriles iban hasta la puerta\n",
      "de las casas, mientras los pasajeros murmuraban\n",
      "que nuestra infancia estaba vacía\n",
      "\n",
      "y la conciencia volvía. \n",
      "<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prueba = \"\"\"El tren repiqueteaba,\n",
    "andaba lento, suave,\n",
    "meciendo a los pasajeros\n",
    "que parados, veían viajar al paisaje\n",
    "\"\"\"\n",
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:00<00:00, 60.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "que penetraba\n",
      "comenzaba a hacerlo\n",
      "con una sensación de no haber logrado ver el día\n",
      "que no era el\n",
      "697, porque en cada parada se había\n",
      "tenido la situación en\n",
      "derruido; y era ese frío\n",
      "que en el tiempo, en el espacio,\n",
      "ha herido los nervios y ha desaparecido el miedo. \n",
      "\n",
      "Mis ojos se fijaban en el paisaje:\n",
      "el paisaje tenía una belleza diferente\n",
      "por<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:00<00:00, 60.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "y en el parabrisas\n",
      "el relente que producía el relente\n",
      "en el morro de un árbol. \n",
      "\n",
      "En los cruces de la Plaza de España,\n",
      "Neptuno como el ave que juguetea,\n",
      "escribiendo la pintura en azul\n",
      "y el papel de un niño pequeño\n",
      "de rodillas, de rodillas,\n",
      "nuestros pies parecen enfermos\n",
      "la cabeza está sobre la cama,\n",
      "la vista en el espejo\n",
      "al<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:02<00:00, 62.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "que, como niños, iban en la misma dirección. \n",
      "\n",
      "Ahogado en el coche, en el cielo,\n",
      "crece un mundo, una blancura extraña. \n",
      "\n",
      "Helo aquí, en este mundo, es una raza. \n",
      "\n",
      "No puedo detener el tiempo, pero este tren me aguarda. \n",
      "\n",
      "Lo noto\n",
      "en el olor. En el aire,\n",
      "en la luz, entre el cielo. \n",
      "\n",
      "Gema se estrem<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:00<00:00, 60.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "A los pasajeros y camareros\n",
      "y unos muchachos con voz\n",
      "realmente infantil. \n",
      "\n",
      "Al alejarse, el vagón crujía\n",
      "Me decía: \n",
      "\n",
      "—¡Baje! —(Y si no se trata de un himno, lo hacen). \n",
      "\n",
      "—Bueno —dijo el conductor—, voy a ver si tienen un ataúd\n",
      "A quién se lo pide. \n",
      "\n",
      "—No, si no, vamos a buscarlo. \n",
      "\n",
      "Todos, al mismo<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_auto_class',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_can_retrieve_inputs_from_name',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_create_or_get_repo',\n",
       " '_expand_inputs_for_generation',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_repo_url_from_name',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_input_ids_for_generation',\n",
       " '_prepare_model_inputs',\n",
       " '_push_to_hub',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'compute_transition_beam_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'constrained_beam_search',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'deparallelize',\n",
       " 'device',\n",
       " 'device_map',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_parallelizable',\n",
       " 'lm_head',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'main_input_name',\n",
       " 'model_parallel',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parallelize',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'transformer',\n",
       " 'type',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM8BMkAIR+smXXMuirnyW5n",
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
