{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661020893946,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "ttSG4wFVEata"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dleir\\miniconda3\\envs\\tfm\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1661020820703,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "WNFp1G8sW_jv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('poems_cleaned.csv')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "test_set = df.sample(n = 200)\n",
    "df = df.loc[~df.index.isin(test_set.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661020821016,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "AbGe08klgmsI"
   },
   "outputs": [],
   "source": [
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1661020821352,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "4Gjs8Mk7gqaV"
   },
   "outputs": [],
   "source": [
    "test_set['True_end_poems'] = test_set['poem'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['poem'] = test_set['poem'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67853,
     "status": "ok",
     "timestamp": 1661020890183,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "XHdeV_N5hO5Q",
    "outputId": "6a7c7b5e-522a-44d2-bf0b-d16c29cfca2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 821k/821k [00:00<00:00, 884kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 487k/487k [00:00<00:00, 1.13MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 262/262 [00:00<00:00, 262kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 115/115 [00:00<00:00, 115kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 914/914 [00:00<00:00, 911kB/s]\n"
     ]
    }
   ],
   "source": [
    "class SpanishPoems(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"DeepESP/gpt2-spanish\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.poems = []\n",
    "\n",
    "        for row in df['poem']:\n",
    "          self.poems.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))               \n",
    "        if truncate:\n",
    "            self.poems = self.poems[:20000]\n",
    "        self.poems_count = len(self.poems)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.poems_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.poems[item]\n",
    "    \n",
    "dataset = SpanishPoems(df['poem'], truncate=True, gpt2_type=\"DeepESP/gpt2-spanish\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2950,
     "status": "ok",
     "timestamp": 1661020893094,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "dIbrayvfi77s",
    "outputId": "b89d8af3-9982-4943-82a4-c26bd45804a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 249M/249M [01:14<00:00, 3.51MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('DeepESP/gpt2-spanish')\n",
    "model = GPT2LMHeadModel.from_pretrained('DeepESP/gpt2-spanish')\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1661020903194,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "ZimJ0dmcjQ5Y"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=5, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\"./spanish_poems_model_0.2\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6903807,
     "status": "ok",
     "timestamp": 1661027825548,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "vnrP1RdWj5T4",
    "outputId": "5805598d-4c86-4ca5-fa1a-346fc5ff11ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dleir\\miniconda3\\envs\\tfm\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [16:07, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(1.5059, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [22:33, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [2:32:29,  2.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [15:28, 21.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(1.2472, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [15:29, 21.53it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2352,
     "status": "ok",
     "timestamp": 1661028019723,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "rzZ0qoT8j944"
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"spanish_poems_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1661029762880,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "OiXaY4If_CON"
   },
   "outputs": [],
   "source": [
    "prueba = \"\"\"\n",
    "No sé, dicen que es invierno\n",
    "y que fuera está nevando.\n",
    "Pero aquí sigue siendo verano.\n",
    "\n",
    "Tú que te coronaste en Cotos sin necesidad de vestido,\n",
    "o de trampas en papel de efecto retardado,\n",
    "querías saber lo que me hizo el sol en el Atlántico\n",
    "pero tú te quemaste la espalda\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1661029030334,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "OnO6hMoW_cID"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1661031209641,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "-Q3YzHwpC5dl"
   },
   "outputs": [],
   "source": [
    "def text_generation(text, temperature=1., top_p=0.8, entry_length=30):\n",
    "  x = generate(model.to('cpu'), tokenizer, text, entry_count=1, temperature=temperature, top_p=top_p, entry_length=entry_length)\n",
    "  print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32077,
     "status": "ok",
     "timestamp": 1661031985398,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "6HK_1vutDI7b",
    "outputId": "f487c343-7832-404d-cd2a-4c9ce4d29ff6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:55<00:00, 55.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No sé, dicen que es invierno\n",
      "y que fuera está nevando.\n",
      "Pero aquí sigue siendo verano.\n",
      "\n",
      "Tú que te coronaste en Cotos sin necesidad de vestido,\n",
      "o de trampas en papel de efecto retardado,\n",
      "querías saber lo que me hizo el sol en el Atlántico\n",
      "pero tú te quemaste la espalda\n",
      "de las plumas de oro que esconden los sombreros.\n",
      "Ni siquiera sabes lo que me hice. \n",
      "\n",
      "¡Que Dios te perdone! ¿Qué digo? Yo, por tu parte, me quedo contigo. \n",
      "\n",
      "Ven conmigo, por cierto, pues, adiós. \n",
      "\n",
      "Pero ya no soy quien para pedirte perdón. \n",
      "\n",
      "Entonces\n",
      "lo sé, tú a tu vez, aunque no lo creas,\n",
      "yo te echo de menos,\n",
      "de ahí el pudor que me sube a la garganta.\n",
      "Nunca, sin embargo,\n",
      "sé que no volveré a verte,\n",
      "nunca te olvido de nada. \n",
      "\n",
      "Yo, sin embargo, nunca volveré a verte. \n",
      "\n",
      "Hasta que no te duermas,\n",
      "queda siempre allí, silencioso<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1661032293187,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "JoTFD2i-M1KV"
   },
   "outputs": [],
   "source": [
    "prueba = \"\"\"\n",
    "Es porque aún somos pequeños.\n",
    "\n",
    "Tú no dejabas de mirar por la ventana,\n",
    "como si las golondrinas fueran a bailar por ti.\n",
    "Irradiabas el aura y la luz de la vida\n",
    "la mirada de una niña que ha visto más de lo que quería\n",
    "por esa ventana que mostraba en el atardecer un nuevo alba.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32000,
     "status": "ok",
     "timestamp": 1661032328116,
     "user": {
      "displayName": "David Leirado Maroto",
      "userId": "18301798507957740037"
     },
     "user_tz": -120
    },
    "id": "sXIpQ1nbO9iv",
    "outputId": "56a658c1-f0d8-49fa-c154-4f166f75a340"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:40<00:00, 40.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Es porque aún somos pequeños.\n",
      "\n",
      "Tú no dejabas de mirar por la ventana,\n",
      "como si las golondrinas fueran a bailar por ti.\n",
      "Irradiabas el aura y la luz de la vida\n",
      "la mirada de una niña que ha visto más de lo que quería\n",
      "por esa ventana que mostraba en el atardecer un nuevo alba.\n",
      "La tristeza de la muerte\n",
      "la espera del momento en que la luz es el símbolo de su fe. \n",
      "\n",
      "Tú has sido siempre una niña. \n",
      "\n",
      "Tú no estás ahí. \n",
      "\n",
      "Tú lo has sido siempre. \n",
      "\n",
      "Tú has estado siempre ahí. \n",
      "\n",
      "Tú no estabas ahí. \n",
      "\n",
      "Yo no estabas. \n",
      "\n",
      "Tú no estabas. \n",
      "\n",
      "Yo no estaba. \n",
      "\n",
      "Yo estaba. \n",
      "\n",
      "Yo estaba. \n",
      "\n",
      "Tú estabas. \n",
      "\n",
      "Tú no estabas. \n",
      "\n",
      "Tú no estabas.\n",
      "Tú no estabas.\n",
      "Yo no estabas. \n",
      "\n",
      "Yo no estabas<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=0.9, top_p=0.8, entry_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OKduKsCHO_Xu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "como en un ritual silencioso\n",
      "al que ninguna de ellas había realizado,\n",
      "medíganes, hadas, castas,\n",
      "las delicias de la calle y la vista\n",
      "del paisaje\n",
      "una flor, florecillas de su perfume\n",
      "y un riquísimo florecer. \"\n",
      "Vivía entonces en la plaza,\n",
      "en un palacio cuyas puertas nunca se abrían,\n",
      "cuando el automóvil cruzaba las puertas\n",
      "como un hombre que se desata<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prueba = \"\"\"El tren repiqueteaba,\n",
    "andaba lento, suave,\n",
    "meciendo a los pasajeros\n",
    "que parados, veían viajar al paisaje\n",
    "\"\"\"\n",
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "que nos rodeaba. No quería que nos cruzaran\n",
      "sus ojos. Atrás quedaba, un campo que\n",
      "\n",
      "aprendía ser bosque. \n",
      "\n",
      "Los viajeros pasaban de largo. Nos desplazábamos\n",
      "avanzando. Los que nos seguían\n",
      "todos se perdían. Pero yo,\n",
      "capullo por la nieve\n",
      "y gemía en mis adentros: \n",
      "\n",
      "—¡Ni en las cimas ni en las ramas! \n",
      "\n",
      "Mis pies se habían<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "con los ojos cerrados.\n",
      "Me adelanté a trompicones,\n",
      "de repente se interrumpió el paisaje\n",
      "y con el vaivén de la corriente\n",
      "tengo la frente adherida al pavimento\n",
      "de nubes de nieve y\n",
      "tengo la cabeza contra la ventana\n",
      "y la mirada en el techo\n",
      "y por el cristal\n",
      "todas las figuras que se mueven\n",
      "que se mueven\n",
      "Que giran y vuelan\n",
      "Yo sé que las imágenes se van<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "en vano. El aire,\n",
      "perdido de sus vagones,\n",
      "destrozaba la expresión.\n",
      "Niego, y luego otro.\n",
      "Por fin, el tren se detenía,\n",
      "se detenía. Entonces, un nuevo,\n",
      "un nuevo, estaba en movimiento,\n",
      "buscaba una aguja. \n",
      "\n",
      "Pero ahora yo veía. \n",
      "\n",
      "Ya era la hora. \n",
      "\n",
      "La mañana, la hora\n",
      "\n",
      "de dejar la estación, la hora<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tren repiqueteaba,\n",
      "andaba lento, suave,\n",
      "meciendo a los pasajeros\n",
      "que parados, veían viajar al paisaje\n",
      "para ver a la luna\n",
      "sus estrellas. En el oscuro túnel,\n",
      "había pájaros, peces, palomas,\n",
      "todos revoloteaban. Y, aunque la luna\n",
      "parecía alta,\n",
      "si llegaba a su destino,\n",
      "me imaginaba a la luna. \n",
      "\n",
      "Estaba ya en el umbral de la estación,\n",
      "en la parte de atrás, en la casa,\n",
      "en las escaleras, y sólo podía verlo. Y entonces, mientras corría\n",
      "bajo<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_generation(prueba, temperature=1., top_p=0.8, entry_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM8BMkAIR+smXXMuirnyW5n",
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
